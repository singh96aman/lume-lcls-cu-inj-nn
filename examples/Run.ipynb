{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0aeb04c",
   "metadata": {},
   "source": [
    "# LUME-services demo\n",
    "In this notebook, we will configure LUME-services to use the service configuration used to launch our docker-compose services. Make sure you've completed all steps outlined in https://slaclab.github.io/lume-services/demo/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b0cfe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)  # Lets check the logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb7294",
   "metadata": {},
   "source": [
    "## Configure services\n",
    "LUME-services is packages with a configuration utility that reads environment variables and initializes services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1e88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lume_services.config:Configuring LUME-services environment...\n",
      "INFO:lume_services.config:Environment configured.\n"
     ]
    }
   ],
   "source": [
    "from lume_services import config\n",
    "config.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14910cc",
   "metadata": {},
   "source": [
    "## if you're running this many time, creation will fail because of uniqueness... You can reset since this is a dev server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1dae5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_db_service._reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ba844",
   "metadata": {},
   "source": [
    "## Create a model\n",
    "The LUME-services Model provides an API to all model services and facilitates all model operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "299e5321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT model.model_id, model.created, model.author, model.laboratory, model.facility, model.beampath, model.description \n",
      "FROM model \n",
      "WHERE model.author = :author_1 AND model.laboratory = :laboratory_1 AND model.facility = :facility_1 AND model.beampath = :beampath_1 AND model.description = :description_1\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT model.model_id, model.created, model.author, model.laboratory, model.facility, model.beampath, model.description \n",
      "FROM model \n",
      "WHERE model.model_id = :model_id_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(metadata=Model(                     model_id=1,                     created=datetime.datetime(2023, 8, 1, 2, 11, 10),                     author='Aman Singh Thakur'),                     laboratory='slac',                     facility='lcls',                     beampath='cu',                     description='lcls-cu-inj-nn'                 ), deployment=None, results=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lume_services.models import Model\n",
    "\n",
    "model = Model.create_model(\n",
    "    author = \"Aman Singh Thakur\",\n",
    "    laboratory = \"slac\",\n",
    "    facility = \"lcls\",\n",
    "    beampath = \"cu\",\n",
    "    description = \"lcls-cu-inj-nn\"\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088ad03",
   "metadata": {},
   "source": [
    "## Create a project\n",
    "Workflows are organized by the Prefect scheduler into different projects. Below, we access the configured services directly (TODO create project registry utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b7a075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_db_service = config.context.model_db_service()\n",
    "scheduling_service = config.context.scheduling_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3c31d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: The below cell will raise an error if run 2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcd2f83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lume_services.services.models.db.db:ModelDB inserting: INSERT INTO project (project_name, description) VALUES (:project_name, :description)\n",
      "INFO:lume_services.services.models.db.db:Sucessfully executed: INSERT INTO project (project_name, description) VALUES (:project_name, :description)\n"
     ]
    }
   ],
   "source": [
    "# create a project\n",
    "project_name = model_db_service.store_project(\n",
    "    project_name=\"gpu-box-new2\", description=\"my_description\"\n",
    ")\n",
    "scheduling_service.create_project(\"gpu-box-new2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbe01a",
   "metadata": {},
   "source": [
    "You can now find this project in you Prefect UI at http://localhost:8080\n",
    "\n",
    "\n",
    "![project](https://slaclab.github.io/lume-services/files/project_nav.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23acff",
   "metadata": {},
   "source": [
    "## Create a deployment for your model\n",
    "Replace `source_path` with the path to your release tarball below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2346c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lume_services.models.model:installing package\n",
      "INFO:lume_services.environment.solver:https://github.com/singh96aman/lume-lcls-cu-inj-nn/releases/download/v0.0.20/lume_lcls_cu_inj_nn-0.0.20.tar.gz saved to /tmp/tmpw0b2t60e/lume_lcls_cu_inj_nn-0.0.20.tar.gz\n",
      "INFO:lume_services.environment.solver:Version 0.0.20 of lume_lcls_cu_inj_nn already installed.\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT deployment.deployment_id, deployment.version, deployment.deploy_date, deployment.package_import_name, deployment.asset_dir, deployment.source, deployment.sha256, deployment.image, deployment.is_live, deployment.model_id \n",
      "FROM deployment \n",
      "WHERE deployment.model_id = :model_id_1 AND deployment.version = :version_1\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT flow.flow_id, flow.flow_name, flow.project_name, flow.deployment_id \n",
      "FROM flow \n",
      "WHERE flow.deployment_id = :deployment_id_1\n",
      "INFO:lume_services.services.scheduling.backends.server:Flow run config is not empty. Clearing existing labels and assigning                     new.\n",
      "INFO:lume_services.services.models.db.db:ModelDB inserting: INSERT INTO flow (flow_id, flow_name, project_name, deployment_id) VALUES (:flow_id, :flow_name, :project_name, :deployment_id)\n",
      "INFO:lume_services.services.models.db.db:Sucessfully executed: INSERT INTO flow (flow_id, flow_name, project_name, deployment_id) VALUES (:flow_id, :flow_name, :project_name, :deployment_id)\n",
      "INFO:lume_services.models.model:Loading deployment 1\n",
      "INFO:lume_services.models.model:Loading deployment 1\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT deployment.deployment_id, deployment.version, deployment.deploy_date, deployment.package_import_name, deployment.asset_dir, deployment.source, deployment.sha256, deployment.image, deployment.is_live, deployment.model_id \n",
      "FROM deployment \n",
      "WHERE deployment.model_id = :model_id_1 AND deployment.deployment_id = :deployment_id_1\n",
      "INFO:lume_services.models.model:Deployment loaded.\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT flow.flow_id, flow.flow_name, flow.project_name, flow.deployment_id \n",
      "FROM flow \n",
      "WHERE flow.deployment_id = :deployment_id_1\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT project.project_name, project.description \n",
      "FROM project \n",
      "WHERE project.project_name = :project_name_1\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT flow_of_flows._id, flow_of_flows.parent_flow_id, flow_of_flows.flow_id, flow_of_flows.position \n",
      "FROM flow_of_flows \n",
      "WHERE flow_of_flows.parent_flow_id = :parent_flow_id_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow URL: http://localhost:8080/default/flow/8b084a95-27d1-4c9f-ab78-44b4ff31a1ae\n",
      " └── ID: 7f896d32-d0b1-4a25-9f3b-9a4afe1d5301\n",
      " └── Project: gpu-box-new2\n",
      " └── Labels: ['lume-services']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(metadata=Model(                     model_id=1,                     created=datetime.datetime(2023, 8, 1, 2, 11, 10),                     author='Aman Singh Thakur'),                     laboratory='slac',                     facility='lcls',                     beampath='cu',                     description='lcls-cu-inj-nn'                 ), deployment=Deployment(metadata=Deployment(                 deployment_id=1,                 model_id=1,                 version='0.0.20',                 deploy_date=datetime.datetime(2023, 8, 1, 2, 11, 56)),                 asset_dir=None,                 source='https://github.com/singh96aman/lume-lcls-cu-inj-nn/releases/download/v0.0.20/lume_lcls_cu_inj_nn-0.0.20.tar.gz',                 sha256='ddfa081fef58a639fe132ef02327a3eb98556e9bea4478ce38c211c414b60153',                 image='scr.svc.stanford.edu/aman96/lume-lcls-cu-inj-nn:v0.0.20',                 is_live=True                 package_import_name='lume_lcls_cu_inj_nn'                 ), project=Project(metadata=Project(                 project_name='gpu-box-new2',                 description='my_description',                 )), flow=Flow(name='lume-lcls-cu-inj-nn', flow_id='7f896d32-d0b1-4a25-9f3b-9a4afe1d5301', project_name='gpu-box-new2', prefect_flow=<Flow: name=\"lume-lcls-cu-inj-nn\">, parameters=None, mapped_parameters=None, task_slugs=None, labels=['lume-services'], image='scr.svc.stanford.edu/aman96/lume-lcls-cu-inj-nn:v0.0.20'), model_type=<class 'lume_lcls_cu_inj_nn.model.LCLSCuInjNN'>), results=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#source_path = \"https://github.com/jacquelinegarrahan/lume-lcls-cu-inj-nn/releases/download/v0.0.12/lume_lcls_cu_inj_nn-0.0.12.tar.gz\"\n",
    "\n",
    "source_path = \"https://github.com/singh96aman/lume-lcls-cu-inj-nn/releases/download/v0.0.20/lume_lcls_cu_inj_nn-0.0.20.tar.gz\"\n",
    "\n",
    "# populates local channel\n",
    "model.store_deployment(source_path, project_name=\"gpu-box-new2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f0c1c",
   "metadata": {},
   "source": [
    "## Run the Prefect workflow directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "514e0aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.FlowRunner | Beginning Flow run for 'lume-lcls-cu-inj-nn'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.FlowRunner:Beginning Flow run for 'lume-lcls-cu-inj-nn'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'configure_lume_services': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'configure_lume_services': Starting task run...\n",
      "INFO:lume_services.config:Configuring LUME-services environment...\n",
      "INFO:lume_services.config:Environment configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'configure_lume_services': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'configure_lume_services': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'L0A_phase:dtheta0_deg': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'L0A_phase:dtheta0_deg': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'L0A_phase:dtheta0_deg': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'L0A_phase:dtheta0_deg': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'distgen:t_dist:length:value': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'distgen:t_dist:length:value': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'distgen:t_dist:length:value': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'distgen:t_dist:length:value': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'SOL1:solenoid_field_scale': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'SOL1:solenoid_field_scale': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'SOL1:solenoid_field_scale': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'SOL1:solenoid_field_scale': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'L0A_scale:voltage': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'L0A_scale:voltage': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'L0A_scale:voltage': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'L0A_scale:voltage': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'check_local_execution': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'check_local_execution': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'check_local_execution': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'check_local_execution': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'distgen:total_charge:value': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'distgen:total_charge:value': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'distgen:total_charge:value': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'distgen:total_charge:value': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'CQ01:b1_gradient': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'CQ01:b1_gradient': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'CQ01:b1_gradient': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'CQ01:b1_gradient': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'end_mean_z': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'end_mean_z': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'end_mean_z': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'end_mean_z': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'case(False)': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'case(False)': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'case(False)': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'case(False)': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'SQ01:b1_gradient': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'SQ01:b1_gradient': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'SQ01:b1_gradient': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'SQ01:b1_gradient': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'distgen:r_dist:sigma_xy:value': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'distgen:r_dist:sigma_xy:value': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'distgen:r_dist:sigma_xy:value': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'distgen:r_dist:sigma_xy:value': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'List': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'List': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'List': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'List': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'Dict': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'Dict': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'Dict': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'Dict': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'prepare_lume_model_variables': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'prepare_lume_model_variables': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'prepare_lume_model_variables': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'prepare_lume_model_variables': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'preprocessing_task': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'preprocessing_task': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'preprocessing_task': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'preprocessing_task': Finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:29-0700] INFO - prefect.TaskRunner | Task 'evaluate': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'evaluate': Starting task run...\n",
      "2023-07-31 19:12:30.062966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:30-0700] INFO - prefect.TaskRunner | [array([[ 1.37666653e-01,  1.23746245e+01,  4.00544158e-05,\n",
      "         8.74263114e-02,  3.52380952e-03,  3.52380952e-03,\n",
      "        -8.89970000e+00,  7.00000000e+13,  4.61470020e+00]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 19:12:30.063175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.078830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.079075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.079254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.079424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.201262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.201813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.202101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.202365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.202622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.202879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.719907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.720128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.720312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.720485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.720654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.720805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 523 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2a:00.0, compute capability: 8.6\n",
      "2023-07-31 19:12:30.721096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 19:12:30.721238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46243 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:61:00.0, compute capability: 8.6\n",
      "INFO:prefect.TaskRunner:[array([[ 1.37666653e-01,  1.23746245e+01,  4.00544158e-05,\n",
      "         8.74263114e-02,  3.52380952e-03,  3.52380952e-03,\n",
      "        -8.89970000e+00,  7.00000000e+13,  4.61470020e+00]])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:31-0700] ERROR - prefect.TaskRunner | Task 'evaluate': Exception encountered during task execution!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/task_runner.py\", line 880, in get_task_run_state\n",
      "    value = prefect.utilities.executors.run_task_with_timeout(\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/utilities/executors.py\", line 468, in run_task_with_timeout\n",
      "    return task.run(*args, **kwargs)  # type: ignore\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/lume_lcls_cu_inj_nn/flow.py\", line 89, in evaluate\n",
      "    return model.evaluate(formatted_input_vars)\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/lume_model/keras/__init__.py\", line 91, in evaluate\n",
      "    model_output = self._model.predict(formatted_input)\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.InternalError: Graph execution error:\n",
      "\n",
      "Detected at node 'model_5/dense_60/MatMul' defined at (most recent call last):\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "      app.start()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "      self.io_loop.start()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "      self._run_once()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "      handle._run()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "      await self.process_one()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "      await dispatch(*args)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "      await result\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "      reply_content = await reply_content\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "      res = shell.run_cell(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "      return super().run_cell(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "      result = self._run_cell(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "      result = runner(coro)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "      if await self.run_code(code, result, async_=asy):\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"/tmp/ipykernel_1932505/433827777.py\", line 1, in <module>\n",
      "      flow_run = model.deployment.flow.prefect_flow.run(**{\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/core/flow.py\", line 1282, in run\n",
      "      state = self._run(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/core/flow.py\", line 1087, in _run\n",
      "      flow_state = runner.run(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/flow_runner.py\", line 276, in run\n",
      "      state = self.get_flow_run_state(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/utilities/executors.py\", line 84, in inner\n",
      "      return runner_method(self, *args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/runner.py\", line 48, in inner\n",
      "      new_state = method(self, state, *args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/flow_runner.py\", line 618, in get_flow_run_state\n",
      "      task_states[task] = executor.submit(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/executors/local.py\", line 28, in submit\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/flow_runner.py\", line 777, in run_task\n",
      "      return task_runner.run(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/utilities/executors.py\", line 570, in wrapper\n",
      "      return func(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/task_runner.py\", line 297, in run\n",
      "      state = self.get_task_run_state(state, inputs=task_inputs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/runner.py\", line 48, in inner\n",
      "      new_state = method(self, state, *args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/task_runner.py\", line 880, in get_task_run_state\n",
      "      value = prefect.utilities.executors.run_task_with_timeout(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/utilities/executors.py\", line 468, in run_task_with_timeout\n",
      "      return task.run(*args, **kwargs)  # type: ignore\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/lume_lcls_cu_inj_nn/flow.py\", line 89, in evaluate\n",
      "      return model.evaluate(formatted_input_vars)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/lume_model/keras/__init__.py\", line 91, in evaluate\n",
      "      model_output = self._model.predict(formatted_input)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 2382, in predict\n",
      "      tmp_batch_outputs = self.predict_function(iterator)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 2169, in predict_function\n",
      "      return step_function(self, iterator)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 2155, in step_function\n",
      "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 2143, in run_step\n",
      "      outputs = model.predict_step(data)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in predict_step\n",
      "      return self(x, training=False)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 558, in __call__\n",
      "      return super().__call__(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/functional.py\", line 512, in call\n",
      "      return self._run_internal_graph(inputs, training=training, mask=mask)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n",
      "      outputs = node.layer(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/layers/core/dense.py\", line 241, in call\n",
      "      outputs = tf.matmul(a=inputs, b=self.kernel)\n",
      "Node: 'model_5/dense_60/MatMul'\n",
      "Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\t [[{{node model_5/dense_60/MatMul}}]] [Op:__inference_predict_function_726]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 19:12:31.441201: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error\n",
      "2023-07-31 19:12:31.441240: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:222] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2023-07-31 19:12:31.441255: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:621 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "2023-07-31 19:12:31.441274: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\t [[{{node model_5/dense_60/MatMul}}]]\n",
      "ERROR:prefect.TaskRunner:Task 'evaluate': Exception encountered during task execution!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/task_runner.py\", line 880, in get_task_run_state\n",
      "    value = prefect.utilities.executors.run_task_with_timeout(\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/utilities/executors.py\", line 468, in run_task_with_timeout\n",
      "    return task.run(*args, **kwargs)  # type: ignore\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/lume_lcls_cu_inj_nn/flow.py\", line 89, in evaluate\n",
      "    return model.evaluate(formatted_input_vars)\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/lume_model/keras/__init__.py\", line 91, in evaluate\n",
      "    model_output = self._model.predict(formatted_input)\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.InternalError: Graph execution error:\n",
      "\n",
      "Detected at node 'model_5/dense_60/MatMul' defined at (most recent call last):\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "      app.start()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "      self.io_loop.start()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "      self._run_once()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "      handle._run()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "      await self.process_one()\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "      await dispatch(*args)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "      await result\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "      reply_content = await reply_content\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "      res = shell.run_cell(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "      return super().run_cell(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "      result = self._run_cell(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "      result = runner(coro)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "      if await self.run_code(code, result, async_=asy):\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"/tmp/ipykernel_1932505/433827777.py\", line 1, in <module>\n",
      "      flow_run = model.deployment.flow.prefect_flow.run(**{\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/core/flow.py\", line 1282, in run\n",
      "      state = self._run(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/core/flow.py\", line 1087, in _run\n",
      "      flow_state = runner.run(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/flow_runner.py\", line 276, in run\n",
      "      state = self.get_flow_run_state(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/utilities/executors.py\", line 84, in inner\n",
      "      return runner_method(self, *args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/runner.py\", line 48, in inner\n",
      "      new_state = method(self, state, *args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/flow_runner.py\", line 618, in get_flow_run_state\n",
      "      task_states[task] = executor.submit(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/executors/local.py\", line 28, in submit\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/flow_runner.py\", line 777, in run_task\n",
      "      return task_runner.run(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/utilities/executors.py\", line 570, in wrapper\n",
      "      return func(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/task_runner.py\", line 297, in run\n",
      "      state = self.get_task_run_state(state, inputs=task_inputs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/runner.py\", line 48, in inner\n",
      "      new_state = method(self, state, *args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/engine/task_runner.py\", line 880, in get_task_run_state\n",
      "      value = prefect.utilities.executors.run_task_with_timeout(\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/prefect/utilities/executors.py\", line 468, in run_task_with_timeout\n",
      "      return task.run(*args, **kwargs)  # type: ignore\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/lume_lcls_cu_inj_nn/flow.py\", line 89, in evaluate\n",
      "      return model.evaluate(formatted_input_vars)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/lume_model/keras/__init__.py\", line 91, in evaluate\n",
      "      model_output = self._model.predict(formatted_input)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 2382, in predict\n",
      "      tmp_batch_outputs = self.predict_function(iterator)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 2169, in predict_function\n",
      "      return step_function(self, iterator)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 2155, in step_function\n",
      "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 2143, in run_step\n",
      "      outputs = model.predict_step(data)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in predict_step\n",
      "      return self(x, training=False)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/training.py\", line 558, in __call__\n",
      "      return super().__call__(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/functional.py\", line 512, in call\n",
      "      return self._run_internal_graph(inputs, training=training, mask=mask)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n",
      "      outputs = node.layer(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/home/thakur12/.conda/envs/lume-lcls-cu-inj-nn-torch/lib/python3.9/site-packages/keras/layers/core/dense.py\", line 241, in call\n",
      "      outputs = tf.matmul(a=inputs, b=self.kernel)\n",
      "Node: 'model_5/dense_60/MatMul'\n",
      "Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\t [[{{node model_5/dense_60/MatMul}}]] [Op:__inference_predict_function_726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:31-0700] INFO - prefect.TaskRunner | Task 'evaluate': Finished task run for task with final state: 'Failed'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'evaluate': Finished task run for task with final state: 'Failed'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:31-0700] INFO - prefect.TaskRunner | Task 'format_result': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'format_result': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:31-0700] INFO - prefect.TaskRunner | Task 'format_result': Finished task run for task with final state: 'TriggerFailed'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'format_result': Finished task run for task with final state: 'TriggerFailed'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:31-0700] INFO - prefect.TaskRunner | Task 'save_db_result': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'save_db_result': Starting task run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:31-0700] INFO - prefect.TaskRunner | Task 'save_db_result': Finished task run for task with final state: 'TriggerFailed'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.TaskRunner:Task 'save_db_result': Finished task run for task with final state: 'TriggerFailed'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 19:12:31-0700] INFO - prefect.FlowRunner | Flow run FAILED: some reference tasks failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prefect.FlowRunner:Flow run FAILED: some reference tasks failed.\n"
     ]
    }
   ],
   "source": [
    "flow_run = model.deployment.flow.prefect_flow.run(**{\n",
    "                        \"distgen:r_dist:sigma_xy:value\": 0.4130, \n",
    "                        \"distgen:total_charge:value\": 250.0, \n",
    "                        \"distgen:t_dist:length:value\":7.499772441611215, \n",
    "                        \"SOL1:solenoid_field_scale\": 0.17, \n",
    "                        \"CQ01:b1_gradient\":-0.0074,\n",
    "                        \"SQ01:b1_gradient\": -0.0074,\n",
    "                        \"L0A_phase:dtheta0_deg\": -8.8997,\n",
    "                        \"L0A_scale:voltage\": 70000000.0,\n",
    "                        \"distgen:t_dist:length:value\": 7.499772441611215,\n",
    "                        \"end_mean_z\": 4.6147002\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc08d87",
   "metadata": {},
   "source": [
    "## Run the workflow inside the service cluster\n",
    "We can use the model interface to directly deploy workflows. When sourcing our environment (`docs/examples/demo.env`), we defined a mount point for the file system using the alias `/lume-services/data`. Let's kick off this workflow and save the file output to that directory. \n",
    "After running the next cell, you'll be able to see the running container in your docker desktop and examine the flow using the Prefect UI at http://localhost:8080/default?flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90a5769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run(\n",
    "    parameters = {\n",
    "                        \"distgen:r_dist:sigma_xy:value\": 0.4130, \n",
    "                        \"distgen:total_charge:value\": 250.0, \n",
    "                        \"distgen:t_dist:length:value\":7.499772441611215, \n",
    "                        \"SOL1:solenoid_field_scale\": 0.17, \n",
    "                        \"CQ01:b1_gradient\":-0.0074,\n",
    "                        \"SQ01:b1_gradient\": -0.0074,\n",
    "                        \"L0A_phase:dtheta0_deg\": -8.8997,\n",
    "                        \"L0A_scale:voltage\": 70000000.0,\n",
    "                        \"distgen:t_dist:length:value\": 7.499772441611215,\n",
    "                        \"end_mean_z\": 4.6147002\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49ca124d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flow(name='lume-lcls-cu-inj-nn', flow_id='7f896d32-d0b1-4a25-9f3b-9a4afe1d5301', project_name='gpu-box-new2', prefect_flow=<Flow: name=\"lume-lcls-cu-inj-nn\">, parameters=None, mapped_parameters=None, task_slugs=None, labels=['lume-services'], image='scr.svc.stanford.edu/aman96/lume-lcls-cu-inj-nn:v0.0.20')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.deployment.flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409851f9",
   "metadata": {},
   "source": [
    "# Get results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ffa8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.get_results()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b388faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = model.get_results_df()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f26a31",
   "metadata": {},
   "source": [
    "## Load model using model id\n",
    "Once your model has been registered, you can use the `Model` api object to interact with your model without running the above registration steps. Let's load a new model object using the model_id we registered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ef8e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT model.model_id, model.created, model.author, model.laboratory, model.facility, model.beampath, model.description \n",
      "FROM model \n",
      "WHERE model.model_id = :model_id_1\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload \n",
    "import lume_services\n",
    "reload(lume_services.models)\n",
    "from lume_services.models import Model\n",
    "\n",
    "#model_id = model.metadata.model_id\n",
    "loaded_model = Model(model_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a15668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(                     model_id=1,                     created=datetime.datetime(2023, 8, 1, 0, 47, 35),                     author='Aman Singh Thakur'),                     laboratory='slac',                     facility='lcls',                     beampath='cu',                     description='lcls-cu-inj-nn'                 )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca3ce4",
   "metadata": {},
   "source": [
    "## Load existing model object\n",
    "Loading a model using the load_deployment method without passing a deployment_id will load the latest deployment registered for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "040bb4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lume_services.models.model:Loading latest deployment.\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT deployment.deployment_id, deployment.version, deployment.deploy_date, deployment.package_import_name, deployment.asset_dir, deployment.source, deployment.sha256, deployment.image, deployment.is_live, deployment.model_id \n",
      "FROM deployment \n",
      "WHERE deployment.model_id = :model_id_1 ORDER BY deployment.deploy_date DESC\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT flow.flow_id, flow.flow_name, flow.project_name, flow.deployment_id \n",
      "FROM flow \n",
      "WHERE flow.deployment_id = :deployment_id_1\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT project.project_name, project.description \n",
      "FROM project \n",
      "WHERE project.project_name = :project_name_1\n",
      "INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT flow_of_flows._id, flow_of_flows.parent_flow_id, flow_of_flows.flow_id, flow_of_flows.position \n",
      "FROM flow_of_flows \n",
      "WHERE flow_of_flows.parent_flow_id = :parent_flow_id_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Deployment(metadata=Deployment(                 deployment_id=1,                 model_id=1,                 version='0.0.20',                 deploy_date=datetime.datetime(2023, 8, 1, 0, 49, 15)),                 asset_dir=None,                 source='https://github.com/singh96aman/lume-lcls-cu-inj-nn/releases/download/v0.0.20/lume_lcls_cu_inj_nn-0.0.20.tar.gz',                 sha256='ddfa081fef58a639fe132ef02327a3eb98556e9bea4478ce38c211c414b60153',                 image='scr.svc.stanford.edu/aman96/lume-lcls-cu-inj-nn:v0.0.20',                 is_live=True                 package_import_name='lume_lcls_cu_inj_nn'                 ), project=Project(metadata=Project(                 project_name='gpu-box-new',                 description='my_description',                 )), flow=Flow(name='lume-lcls-cu-inj-nn', flow_id='a4712eac-e30a-4abc-b6e9-56e0c706be8e', project_name='gpu-box-new', prefect_flow=None, parameters=None, mapped_parameters=None, task_slugs=None, labels=['lume-services'], image='scr.svc.stanford.edu/aman96/lume-lcls-cu-inj-nn:v0.0.20'), model_type=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.load_deployment()\n",
    "loaded_model.deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = loaded_model.run_and_return(\n",
    "    parameters={\n",
    "                        \"distgen:r_dist:sigma_xy:value\": 0.4130, \n",
    "                        \"distgen:total_charge:value\": 250.0, \n",
    "                        \"distgen:t_dist:length:value\":7.499772441611215, \n",
    "                        \"SOL1:solenoid_field_scale\": 0.17, \n",
    "                        \"CQ01:b1_gradient\":-0.0074,\n",
    "                        \"SQ01:b1_gradient\": -0.0074,\n",
    "                        \"L0A_phase:dtheta0_deg\": -8.8997,\n",
    "                        \"L0A_scale:voltage\": 70000000.0,\n",
    "                        \"distgen:t_dist:length:value\": 7.499772441611215,\n",
    "                        \"end_mean_z\": 4.6147002\n",
    "    },\n",
    "    task_name=\"save_db_result\" # Want to get the result from the save_db_result task\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c25a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = loaded_model.get_results_df()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a3f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b1aa5d32c5360a08f3d071b8537e8759b8901de3eea112666c46651e79793b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
